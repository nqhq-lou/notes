# HPC
- high performance computer
- 每周工作时间: Tue->晚, Sat->上午

## Schedule
- 目标:
- 运维1: 软件安装, 系统操作
	- 在已有的nodes上进行 包安装软件/依赖管理 和运行
	- 熟悉node间同步设置和操作方法
    - 运维2: 服务相关
        - 指定服务器管理规则: 文件夹结构, 用户权限, 存储划分
        - 远程的图形界面
- 软件1: 多线程编程
	- 使用本地编译器编译MPI程序, 验证效果
	- :question:自学某语言MPI编程
    - 软件2: 分布式计算适用软件
        - Comsol: 分布式计算版本, CUPT的同学们可以用到
        - lammps: 分子动力学
        - 根据热统I题目, 学习相关软件使用
- 硬件1: 熟悉配置过程
	- 尝试解决login/每个node网络连接的问题
		- 魏老师给出的解决方法
	- 阅读`/public/home`路径下
		- `adminhistory.bak`, `loginhistory.bak`, `node1history.bak`
	- 理解安装过程
    - 硬件2: 向节点加入更多node
        - 软件: 安装包 & 安装
        - 硬件: 连接线 & 供电功率 & 散热情况
    - Master:
        - 尝试安装和运行centOS最新版的系统(或其他发行版)
        - 关键问题: 硬件适配, PBS配置
- 时间线:
    - 03.20 ~ 04.30:
        - 运维1, 硬件1, 软件1
		- 0418: (未完成)多线程编程
    - 05.01 ~ 06.18:
		- 调整: 增加node推迟, 先用已有的
        - 运维2, 硬件2, 软件2
    - 期末考试后 ~ 暑假期间:
        - (尝试Master)

## Intro
- 分布式 distributed
	- 有些商用软件提供分布式的版本
		- 大都是有限元方向
	- 开放软件: [lammps](https://lammps.sandia.gov/)
		- [lammps官方中文教程](https://lammps.org.cn/)
		- [zhihu: lammps安装](https://zhuanlan.zhihu.com/p/34555369)
- **分子动力学**, molecular dynamics
	- 适合用机群来做(分布式的特点)
		- 可以把规模做得大一点
	- **先确定要做什么, 然后根据需要去改进环境**

## 关于集群
- login, admin
		- `10.92.3.111`
- node情况
	- 有1-10, 6坏掉
		- 每个节点都有24GB内存, 100GB存储
		- 外置11TB硬盘
		- 还有11-20, 21-30
	- 版本为
		- CentOS 6.9
		- 曙光DAWNING?一个公司, 生产商
			- [中科曙光](https://www.sugon.com/download/lists?zl=1&product_model=ParaStor300S&zl=3)
	- 曾经安装的指令的备份, 可以作为参考
- 互联使用IB, infinite band?
	- 调度软件PBS, NASA出的
	- 可以用于?
	- 其他的通信协议: openMPI, intelMPI
		- 不同程序需要依赖不同的通讯协议
- 内部互联
	- login管理用户, admin装软件
	- 网络通讯更快

## 目标
- 需要一个使用说明, 对不同使用群体
	- 对新用户的
	- 对管理员的
- 先参照: 别人的成果, 一个成熟的机群是怎么做说明书的
- 可视界面
	- 使用VNC

## 笔记
- OpenMP：
	- 在节点内(多核SMP)执行的基于共享内存的编程模型
	- OpenMP是针对单主机上多核/多CPU并行计算而设计的工具
		- 它在多核/多CPU结构上的效率很高, 内存开销小, 编程语句简洁直观, 因此编程容易, 编译器实现也容易
		- 不过OpenMP最大的缺点是只能在单台主机上工作, 不能用于多台主机间的并行计算
- OpenMP
- `MPI` ＝ `message passing interface`
	- 在分布式内存（distributed-memory）之间实现信息通讯的一种 规范/标准/协议（standard）。它是一个库，不是一门语言。可以被fortran，c，c++等调用。MPI 允许静态任务调度，显示并行提供了良好的性能和移植性，用 MPI 编写的程序可直接在多核集群上运行。在集群系统中，集群的各节点之间可以采用 MPI 编程模型进行程序设计，每个节点都有自己的内存，可以对本地的指令和数据直接进行访问，各节点之间通过互联网络进行消息传递，这样设计具有很好的可移植性，完备的异步通信功能，较强的可扩展性等优点。MPI 模型存在一些不足，包括：程序的分解、开发和调试相对困难，而且通常要求对代码做大量的改动；通信会造成很大的开销，为了最小化延迟，通常需要大的代码粒度；细粒度的并行会引发大量的通信；动态负载平衡困难；并行化改进需要大量地修改原有的串行代码，调试难度比较大
- MPICH和OpenMPI:
	- 它们都是采用MPI标准，在并行计算中，实现节点间通信的开源软件。各自有各自的函数，指令和库.
		- MPICH2是MPICH的一个版本.
	- 有的计算机厂商，也会针对旗下机型特点，自主开发基于MPICH的MPI软件，从而使机器的并行计算效率得以提高.
	- 比较简单，修改现有的大段代码也容易。基本上openmp只要在已有程序基础上根据需要加并行语句即可。而mpi有时甚至需要从基本设计思路上重写整个程序，调试也困难得多，涉及到局域网通信这一不确定的因素。不过，openmp虽然简单却只能用于单机多CPU/多核并行，mpi才是用于多主机超级计算机集群的强悍工具，当然复杂。

